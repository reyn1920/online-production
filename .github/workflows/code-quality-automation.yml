name: Code Quality Automation with Browser Testing

on:
  push:
    branches: [ main, develop ]
  pull_request:
    branches: [ main ]
  workflow_dispatch:
    inputs:
      environment:
        description: 'Target environment'
        required: true
        default: 'staging'
        type: choice
        options:
        - staging
        - production
      run_browser_tests:
        description: 'Run browser automation tests'
        required: false
        default: true
        type: boolean

env:
  NODE_VERSION: '18'
  PYTHON_VERSION: '3.11'

jobs:
  # Static Code Analysis
  static-analysis:
    name: Static Code Analysis
    runs-on: ubuntu-latest
    steps:
    - name: Checkout code
      uses: actions/checkout@v4
      with:
        fetch-depth: 0

    - name: Setup Python
      uses: actions/setup-python@v4
      with:
        python-version: ${{ env.PYTHON_VERSION }}

    - name: Install Python dependencies
      run: |
        python -m pip install --upgrade pip
        pip install flake8 pylint bandit safety
        if [ -f requirements.txt ]; then pip install -r requirements.txt; fi

    - name: Run Flake8 linting
      run: |
        flake8 . --count --select=E9,F63,F7,F82 --show-source --statistics
        flake8 . --count --exit-zero --max-complexity=10 --max-line-length=127 --statistics

    - name: Run Pylint analysis
      run: |
        pylint --exit-zero --output-format=text $(find . -name "*.py" | head -20)

    - name: Run Bandit security scan
      run: |
        bandit -r . -f json -o bandit-report.json || true
        bandit -r . -f txt

    - name: Check for known security vulnerabilities
      run: |
        safety check --json --output safety-report.json || true
        safety check

    - name: Upload security reports
      uses: actions/upload-artifact@v3
      if: always()
      with:
        name: security-reports
        path: |
          bandit-report.json
          safety-report.json

  # Browser Automation Tests
  browser-automation:
    name: Browser Automation & Web Validation
    runs-on: ubuntu-latest
    if: github.event.inputs.run_browser_tests != 'false'
    needs: static-analysis
    steps:
    - name: Checkout code
      uses: actions/checkout@v4

    - name: Setup Node.js
      uses: actions/setup-node@v4
      with:
        node-version: ${{ env.NODE_VERSION }}
        cache: 'npm'

    - name: Setup Python
      uses: actions/setup-python@v4
      with:
        python-version: ${{ env.PYTHON_VERSION }}

    - name: Install dependencies
      run: |
        npm ci
        python -m pip install --upgrade pip
        if [ -f requirements.txt ]; then pip install -r requirements.txt; fi

    - name: Install Playwright browsers
      run: |
        npx playwright install --with-deps chromium

    - name: Build application
      run: npm run build

    - name: Start development server
      run: |
        npm run dev &
        sleep 10
      env:
        CI: true

    - name: Run Puppeteer code quality automation
      run: |
        python -c "
        import asyncio
        import sys
        import os
        sys.path.append('rewritten/integrations')
        
        async def run_quality_checks():
            try:
                from puppeteer_service import PuppeteerService
                from mcp_client import MCPClient
                
                mcp_client = MCPClient()
                service = PuppeteerService(mcp_client)
                
                # Run code quality analysis
                result = await service.run_code_quality_analysis(
                    project_path='.',
                    tools=['flake8', 'pylint', 'bandit']
                )
                
                print('Code Quality Analysis Results:')
                print(f'Overall Score: {result.overall_score}')
                print(f'Issues Found: {len(result.issues)}')
                
                for issue in result.issues[:10]:  # Show first 10 issues
                    print(f'- {issue.severity}: {issue.message} ({issue.file}:{issue.line})')
                
                # Test Netlify deployment preview
                if os.getenv('NETLIFY_AUTH_TOKEN'):
                    deploy_result = await service.test_netlify_deployment(
                        site_id=os.getenv('NETLIFY_SITE_ID', 'test-site'),
                        build_dir='dist'
                    )
                    print(f'Deployment Test: {"PASSED" if deploy_result.success else "FAILED"}')
                    if deploy_result.preview_url:
                        print(f'Preview URL: {deploy_result.preview_url}')
                
                return result.overall_score >= 7.0  # Minimum quality threshold
                
            except Exception as e:
                print(f'Error running quality checks: {e}')
                return False
        
        success = asyncio.run(run_quality_checks())
        sys.exit(0 if success else 1)
        "
      env:
        NETLIFY_AUTH_TOKEN: ${{ secrets.NETLIFY_AUTH_TOKEN }}
        NETLIFY_SITE_ID: ${{ secrets.NETLIFY_SITE_ID }}

    - name: Run web-based code validation
      run: |
        python -c "
        import asyncio
        import sys
        sys.path.append('rewritten')
        
        async def run_web_validation():
            try:
                from python_syntax_error_fixer import PythonSyntaxFixer
                
                fixer = PythonSyntaxFixer()
                
                # Run online validation for Python files
                result = await fixer.validate_project_online('.')
                
                print('Web Validation Results:')
                print(f'Total Files: {result["total_files"]}')
                print(f'Validated Files: {result["validated_files"]}')
                print(f'Errors: {len(result["errors"])}')
                print(f'Warnings: {len(result["warnings"])}')
                
                return len(result['errors']) == 0
                
            except Exception as e:
                print(f'Error running web validation: {e}')
                return True  # Don't fail the build for web validation issues
        
        asyncio.run(run_web_validation())
        "

    - name: Upload browser test results
      uses: actions/upload-artifact@v3
      if: always()
      with:
        name: browser-test-results
        path: |
          test-results/
          screenshots/

  # SonarQube Analysis (if configured)
  sonarqube-analysis:
    name: SonarQube Quality Gate
    runs-on: ubuntu-latest
    if: vars.SONAR_PROJECT_KEY != ''
    needs: [static-analysis, browser-automation]
    steps:
    - name: Checkout code
      uses: actions/checkout@v4
      with:
        fetch-depth: 0

    - name: SonarQube Scan
      uses: sonarqube-quality-gate-action@master
      with:
        scanMetadataReportFile: target/sonar/report-task.txt
      env:
        SONAR_TOKEN: ${{ secrets.SONAR_TOKEN }}
        SONAR_HOST_URL: ${{ vars.SONAR_HOST_URL }}

    - name: SonarQube Quality Gate Check
      run: |
        python -c "
        import asyncio
        import sys
        import os
        sys.path.append('rewritten/integrations')
        
        async def check_sonarqube():
            try:
                from puppeteer_service import PuppeteerService
                from mcp_client import MCPClient
                
                if not os.getenv('SONAR_TOKEN'):
                    print('SonarQube not configured, skipping...')
                    return True
                
                mcp_client = MCPClient()
                service = PuppeteerService(mcp_client)
                
                # Automate SonarQube quality gate validation
                result = await service.validate_sonarqube_quality_gate(
                    project_key=os.getenv('SONAR_PROJECT_KEY'),
                    sonar_url=os.getenv('SONAR_HOST_URL', 'https://sonarcloud.io')
                )
                
                print(f'SonarQube Quality Gate: {"PASSED" if result.passed else "FAILED"}')
                if result.issues:
                    print('Quality Gate Issues:')
                    for issue in result.issues:
                        print(f'- {issue}')
                
                return result.passed
                
            except Exception as e:
                print(f'Error checking SonarQube: {e}')
                return True  # Don't fail build if SonarQube check fails
        
        asyncio.run(check_sonarqube())
        "
      env:
        SONAR_TOKEN: ${{ secrets.SONAR_TOKEN }}
        SONAR_HOST_URL: ${{ vars.SONAR_HOST_URL }}
        SONAR_PROJECT_KEY: ${{ vars.SONAR_PROJECT_KEY }}

  # Deployment (only on main branch or manual trigger)
  deploy:
    name: Deploy Application
    runs-on: ubuntu-latest
    needs: [static-analysis, browser-automation]
    if: github.ref == 'refs/heads/main' || github.event_name == 'workflow_dispatch'
    environment: ${{ github.event.inputs.environment || 'staging' }}
    steps:
    - name: Checkout code
      uses: actions/checkout@v4

    - name: Setup Node.js
      uses: actions/setup-node@v4
      with:
        node-version: ${{ env.NODE_VERSION }}
        cache: 'npm'

    - name: Install dependencies and build
      run: |
        npm ci
        npm run build

    - name: Deploy to Netlify
      uses: nwtgck/actions-netlify@v2.1
      with:
        publish-dir: './dist'
        production-branch: main
        production-deploy: ${{ github.event.inputs.environment == 'production' }}
        github-token: ${{ secrets.GITHUB_TOKEN }}
        deploy-message: "Deploy from GitHub Actions - ${{ github.sha }}"
        enable-pull-request-comment: true
        enable-commit-comment: true
        overwrites-pull-request-comment: true
      env:
        NETLIFY_AUTH_TOKEN: ${{ secrets.NETLIFY_AUTH_TOKEN }}
        NETLIFY_SITE_ID: ${{ secrets.NETLIFY_SITE_ID }}

    - name: Post-deployment validation
      if: success()
      run: |
        python -c "
        import asyncio
        import sys
        import os
        sys.path.append('rewritten/integrations')
        
        async def validate_deployment():
            try:
                from puppeteer_service import PuppeteerService
                from mcp_client import MCPClient
                
                mcp_client = MCPClient()
                service = PuppeteerService(mcp_client)
                
                # Get deployment URL from Netlify
                deploy_url = os.getenv('DEPLOY_URL', 'https://example.netlify.app')
                
                # Run post-deployment health checks
                health_check = await service.run_health_checks(deploy_url)
                
                print(f'Deployment Health Check: {"PASSED" if health_check.healthy else "FAILED"}')
                print(f'Response Time: {health_check.response_time}ms')
                print(f'Status Code: {health_check.status_code}')
                
                if health_check.errors:
                    print('Health Check Errors:')
                    for error in health_check.errors:
                        print(f'- {error}')
                
                return health_check.healthy
                
            except Exception as e:
                print(f'Error validating deployment: {e}')
                return True  # Don't fail deployment for validation issues
        
        asyncio.run(validate_deployment())
        "
      env:
        DEPLOY_URL: ${{ steps.deploy.outputs.deploy-url }}

  # Quality Report Generation
  generate-report:
    name: Generate Quality Report
    runs-on: ubuntu-latest
    needs: [static-analysis, browser-automation, sonarqube-analysis]
    if: always()
    steps:
    - name: Checkout code
      uses: actions/checkout@v4

    - name: Download all artifacts
      uses: actions/download-artifact@v3

    - name: Generate comprehensive quality report
      run: |
        python -c "
        import json
        import os
        from datetime import datetime
        
        report = {
            'timestamp': datetime.now().isoformat(),
            'commit': os.getenv('GITHUB_SHA'),
            'branch': os.getenv('GITHUB_REF_NAME'),
            'workflow_run': os.getenv('GITHUB_RUN_ID'),
            'quality_checks': {
                'static_analysis': 'completed',
                'browser_automation': 'completed' if os.path.exists('browser-test-results') else 'skipped',
                'sonarqube': 'completed' if os.getenv('SONAR_TOKEN') else 'not_configured'
            },
            'artifacts': {
                'security_reports': os.path.exists('security-reports'),
                'browser_results': os.path.exists('browser-test-results')
            }
        }
        
        with open('quality-report.json', 'w') as f:
            json.dump(report, f, indent=2)
        
        print('Quality Report Generated:')
        print(json.dumps(report, indent=2))
        "

    - name: Upload quality report
      uses: actions/upload-artifact@v3
      with:
        name: quality-report
        path: quality-report.json

    - name: Comment PR with quality report
      if: github.event_name == 'pull_request'
      uses: actions/github-script@v6
      with:
        script: |
          const fs = require('fs');
          if (fs.existsSync('quality-report.json')) {
            const report = JSON.parse(fs.readFileSync('quality-report.json', 'utf8'));
            const comment = `## 🔍 Code Quality Report
            
            **Commit:** ${report.commit}
            **Branch:** ${report.branch}
            
            ### Quality Checks Status
            - Static Analysis: ${report.quality_checks.static_analysis}
            - Browser Automation: ${report.quality_checks.browser_automation}
            - SonarQube: ${report.quality_checks.sonarqube}
            
            ### Artifacts Generated
            - Security Reports: ${report.artifacts.security_reports ? '✅' : '❌'}
            - Browser Test Results: ${report.artifacts.browser_results ? '✅' : '❌'}
            
            *Generated at ${report.timestamp}*`;
            
            github.rest.issues.createComment({
              issue_number: context.issue.number,
              owner: context.repo.owner,
              repo: context.repo.repo,
              body: comment
            });
          }