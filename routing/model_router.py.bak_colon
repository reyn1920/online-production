#!/usr/bin/env python3
""""""
Intelligent Model Router for RouteLL API
Routes requests to optimal models based on cost, performance, and task requirements
""""""

import json
import logging
import os
import sys
import time
from dataclasses import dataclass
from datetime import datetime
from enum import Enum
from typing import Dict, List, Tuple

# Add parent directory to path for imports
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

from utils.rate_limiter import CreditOptimizer, RateLimiter


class TaskType(Enum):
    """Different types of AI tasks that may require different models"""

    GENERAL_CHAT = "general_chat"
    CODE_GENERATION = "code_generation"
    ANALYSIS = "analysis"
    CREATIVE_WRITING = "creative_writing"
    SUMMARIZATION = "summarization"
    TRANSLATION = "translation"
    MATH_REASONING = "math_reasoning"
    QUESTION_ANSWERING = "question_answering"


class ModelTier(Enum):
    """Model performance and cost tiers"""

    FREE = "free"  # Unlimited models, no cost
    STANDARD = "standard"  # Balanced cost/performance
    PREMIUM = "premium"  # High - cost, high - performance
    SPECIALIZED = "specialized"  # Task - specific models


@dataclass
class ModelCapability:
    """Represents a model's capabilities and characteristics"""'

    name: str
    tier: ModelTier
    cost_per_token: float
    max_tokens: int
    strengths: List[TaskType]
    avg_response_time: float = 2.0
    quality_score: float = 0.8  # 0 - 1 scale
    availability_score: float = 1.0  # 0 - 1 scale
    context_window: int = 4096


class ModelRouter:
    """"""
    Intelligent router that selects optimal models based on multiple factors
    """"""

    def __init__(self, config_path: str = None, rate_limiter: RateLimiter = None):
        self.config_path = (
            config_path
            or "/Users/thomasbrianreynolds/online production/config/routellm_config.json"
# BRACKET_SURGEON: disabled
#         )
        self.config = self._load_config()
        self.rate_limiter = rate_limiter or RateLimiter()
        self.optimizer = CreditOptimizer(self.rate_limiter)

        # Model registry
        self.models = self._initialize_models()

        # Routing history for learning
        self.routing_history = []
        self.performance_cache = {}

        # Task classification patterns
        self.task_patterns = self._load_task_patterns()

        # Setup logging
        logging.basicConfig(level=logging.INFO)
        self.logger = logging.getLogger(__name__)

        self.logger.info(f"ðŸŽ¯ Model Router initialized with {len(self.models)} models")

    def _load_config(self) -> Dict:
        """Load configuration from file"""
        try:
            with open(self.config_path, "r") as f:
                return json.load(f)
        except Exception as e:
            self.logger.error(f"Failed to load config: {e}")
            return self._get_default_config()

    def _get_default_config(self) -> Dict:
        """Get default configuration"""
        return {
            "model_routing": {
                "enabled": True,
                "learning_enabled": True,
                "fallback_model": "route - llm",
                "max_routing_time": 0.1,
# BRACKET_SURGEON: disabled
#             },
            "credit_system": {
                "unlimited_models": ["llama - 3.1 - 8b", "gemma - 2 - 9b"],
                "high_cost_models": ["gpt - 4", "claude - 3 - opus", "gemini - pro"],
# BRACKET_SURGEON: disabled
#             },
# BRACKET_SURGEON: disabled
#         }

    def _initialize_models(self) -> Dict[str, ModelCapability]:
        """Initialize the model registry with capabilities"""
        models = {}

        # Free/Unlimited models
        unlimited_models = self.config.get("credit_system", {}).get("unlimited_models", [])
        for model in unlimited_models:
            models[model] = ModelCapability(
                name=model,
                tier=ModelTier.FREE,
                cost_per_token=0.0,
                max_tokens=4096,
                strengths=[TaskType.GENERAL_CHAT, TaskType.QUESTION_ANSWERING],
                avg_response_time=2.0,
                quality_score=0.7,
# BRACKET_SURGEON: disabled
#             )

        # Premium models
        premium_models = self.config.get("credit_system", {}).get("high_cost_models", [])
        for model in premium_models:
            models[model] = ModelCapability(
                name=model,
                tier=ModelTier.PREMIUM,
                cost_per_token=0.01,
                max_tokens=8192,
                strengths=[
                    TaskType.CODE_GENERATION,
                    TaskType.ANALYSIS,
                    TaskType.CREATIVE_WRITING,
# BRACKET_SURGEON: disabled
#                 ],
                avg_response_time=3.0,
                quality_score=0.95,
# BRACKET_SURGEON: disabled
#             )

        # Default route - llm model
        models["route - llm"] = ModelCapability(
            name="route - llm",
            tier=ModelTier.STANDARD,
            cost_per_token=0.002,
            max_tokens=4096,
            strengths=[
                TaskType.GENERAL_CHAT,
                TaskType.SUMMARIZATION,
                TaskType.QUESTION_ANSWERING,
# BRACKET_SURGEON: disabled
#             ],
            avg_response_time=1.5,
            quality_score=0.85,
# BRACKET_SURGEON: disabled
#         )

        # Specialized models (if available)
        specialized_models = {
            "code - llama": ModelCapability(
                name="code - llama",
                tier=ModelTier.SPECIALIZED,
                cost_per_token=0.003,
                max_tokens=4096,
                strengths=[TaskType.CODE_GENERATION],
                avg_response_time=2.5,
                quality_score=0.9,
# BRACKET_SURGEON: disabled
#             ),
            "math - llm": ModelCapability(
                name="math - llm",
                tier=ModelTier.SPECIALIZED,
                cost_per_token=0.004,
                max_tokens=2048,
                strengths=[TaskType.MATH_REASONING],
                avg_response_time=3.0,
                quality_score=0.92,
# BRACKET_SURGEON: disabled
#             ),
# BRACKET_SURGEON: disabled
#         }

        models.update(specialized_models)
        return models

    def _load_task_patterns(self) -> Dict[TaskType, List[str]]:
        """Load patterns for task classification"""
        return {
            TaskType.CODE_GENERATION: [
                "write code",
                "function",
                "class",
                "algorithm",
                "debug",
                "programming",
                "python",
                "javascript",
                "java",
                "c++",
                "sql",
                "html",
                "css",
                "react",
# BRACKET_SURGEON: disabled
#             ],
            TaskType.MATH_REASONING: [
                "calculate",
                "solve",
                "equation",
                "mathematics",
                "algebra",
                "geometry",
                "statistics",
                "probability",
                "integral",
                "derivative",
                "theorem",
# BRACKET_SURGEON: disabled
#             ],
            TaskType.CREATIVE_WRITING: [
                "story",
                "poem",
                "creative",
                "narrative",
                "character",
                "plot",
                "write a",
                "compose",
                "imagine",
                "fiction",
                "novel",
# BRACKET_SURGEON: disabled
#             ],
            TaskType.ANALYSIS: [
                "analyze",
                "compare",
                "evaluate",
                "assess",
                "review",
                "examine",
                "pros and cons",
                "advantages",
                "disadvantages",
                "critique",
# BRACKET_SURGEON: disabled
#             ],
            TaskType.SUMMARIZATION: [
                "summarize",
                "summary",
                "brief",
                "overview",
                "key points",
                "main ideas",
                "tldr",
                "condensed",
                "abstract",
# BRACKET_SURGEON: disabled
#             ],
            TaskType.TRANSLATION: [
                "translate",
                "translation",
                "language",
                "french",
                "spanish",
                "german",
                "chinese",
                "japanese",
                "convert to",
# BRACKET_SURGEON: disabled
#             ],
            TaskType.QUESTION_ANSWERING: [
                "what is",
                "how to",
                "why",
                "when",
                "where",
                "who",
                "explain",
                "definition",
                "meaning",
                "help me understand",
# BRACKET_SURGEON: disabled
#             ],
# BRACKET_SURGEON: disabled
#         }

    def classify_task(self, messages: List[Dict]) -> TaskType:
        """Classify the task type based on message content"""
        if not messages:
            return TaskType.GENERAL_CHAT

        # Get the latest user message
        user_messages = [msg for msg in messages if msg.get("role") == "user"]
        if not user_messages:
            return TaskType.GENERAL_CHAT

        content = user_messages[-1].get("content", "").lower()

        # Score each task type
        task_scores = {}
        for task_type, patterns in self.task_patterns.items():
            score = sum(1 for pattern in patterns if pattern in content)
            if score > 0:
                task_scores[task_type] = score

        # Return the highest scoring task type
        if task_scores:
            return max(task_scores.items(), key=lambda x: x[1])[0]

        return TaskType.GENERAL_CHAT

    def estimate_complexity(self, messages: List[Dict]) -> float:
        """Estimate task complexity (0 - 1 scale)"""
        if not messages:
            return 0.3

        user_messages = [msg for msg in messages if msg.get("role") == "user"]
        if not user_messages:
            return 0.3

        content = user_messages[-1].get("content", "")

        # Factors that increase complexity
        complexity_indicators = [
            len(content) > 500,  # Long input
            len(messages) > 10,  # Long conversation
            any(
                word in content.lower()
                for word in ["complex", "detailed", "comprehensive", "thorough"]
# BRACKET_SURGEON: disabled
#             ),
            content.count("?") > 2,  # Multiple questions
            any(
                word in content.lower()
                for word in ["analyze", "compare", "evaluate", "explain in detail"]
# BRACKET_SURGEON: disabled
#             ),
# BRACKET_SURGEON: disabled
#         ]

        base_complexity = 0.3
        complexity_boost = sum(complexity_indicators) * 0.15

        return min(1.0, base_complexity + complexity_boost)

    def route_request(
        self, messages: List[Dict], preferences: Dict = None, constraints: Dict = None
# BRACKET_SURGEON: disabled
#     ) -> Dict:
        """"""
        Route a request to the optimal model

        Args:
            messages: Conversation messages
            preferences: User preferences (cost, speed, quality weights)
            constraints: Hard constraints (max_cost, max_time, required_features)

        Returns:
            Dict with routing decision and optimized parameters
        """"""
        start_time = time.time()

        # Default preferences
        preferences = preferences or {"cost": 0.4, "speed": 0.3, "quality": 0.3}
        constraints = constraints or {}

        # Classify the task
        task_type = self.classify_task(messages)
        complexity = self.estimate_complexity(messages)

        self.logger.info(f"ðŸŽ¯ Routing request: task={task_type.value}, complexity={complexity:.2f}")

        # Get candidate models
        candidates = self._get_candidate_models(task_type, constraints)

        if not candidates:
            # Fallback to default model
            fallback_model = self.config.get("model_routing", {}).get(
                "fallback_model", "route - llm"
# BRACKET_SURGEON: disabled
#             )
            return self._create_routing_result(
                fallback_model, messages, task_type, "No suitable candidates"
# BRACKET_SURGEON: disabled
#             )

        # Score and rank candidates
        scored_candidates = self._score_candidates(
            candidates, task_type, complexity, preferences, constraints
# BRACKET_SURGEON: disabled
#         )

        # Select the best model
        best_model = scored_candidates[0][0]  # (model_name, score)

        # Create optimized parameters
        result = self._create_routing_result(best_model, messages, task_type, "Optimal selection")

        # Record routing decision
        routing_time = time.time() - start_time
        self._record_routing_decision(
            {
                "timestamp": time.time(),
                "task_type": task_type.value,
                "complexity": complexity,
                "selected_model": best_model,
                "candidates_count": len(candidates),
                "routing_time": routing_time,
                "preferences": preferences,
# BRACKET_SURGEON: disabled
#             }
# BRACKET_SURGEON: disabled
#         )

        self.logger.info(f"âœ… Selected model: {best_model} (routing time: {routing_time:.3f}s)")
        return result

    def _get_candidate_models(self, task_type: TaskType, constraints: Dict) -> List[str]:
        """Get candidate models suitable for the task"""
        candidates = []

        max_cost = constraints.get("max_cost_per_token")
        required_context = constraints.get("min_context_window", 0)

        for model_name, model in self.models.items():
            # Check cost constraint
            if max_cost and model.cost_per_token > max_cost:
                continue

            # Check context window constraint
            if model.context_window < required_context:
                continue

            # Check if model is good for this task type
            if task_type in model.strengths or task_type == TaskType.GENERAL_CHAT:
                candidates.append(model_name)

            # Always include free models as candidates
            elif model.tier == ModelTier.FREE:
                candidates.append(model_name)

        return candidates

    def _score_candidates(
        self,
        candidates: List[str],
        task_type: TaskType,
        complexity: float,
        preferences: Dict,
        constraints: Dict,
    ) -> List[Tuple[str, float]]:
        """Score and rank candidate models"""
        scored = []

        for model_name in candidates:
            model = self.models[model_name]
            score = self._calculate_model_score(model, task_type, complexity, preferences)
            scored.append((model_name, score))

        # Sort by score (descending)
        return sorted(scored, key=lambda x: x[1], reverse=True)

    def _calculate_model_score(
        self,
        model: ModelCapability,
        task_type: TaskType,
        complexity: float,
        preferences: Dict,
# BRACKET_SURGEON: disabled
#     ) -> float:
        """Calculate a composite score for a model"""
        # Base scores (0 - 1 scale)
        quality_score = model.quality_score

        # Speed score (inverse of response time, normalized)
        speed_score = max(0, 1 - (model.avg_response_time / 10))

        # Cost score (inverse of cost, normalized)
        if model.cost_per_token == 0:
            cost_score = 1.0  # Free models get perfect cost score
        else:
            cost_score = max(0, 1 - (model.cost_per_token / 0.02))  # Normalize to $0.02/token max

        # Task suitability score
        task_score = 1.0 if task_type in model.strengths else 0.7

        # Complexity adjustment
        if complexity > 0.7 and model.tier in [
            ModelTier.PREMIUM,
            ModelTier.SPECIALIZED,
# BRACKET_SURGEON: disabled
#         ]:
            quality_score *= 1.2  # Boost premium models for complex tasks
        elif complexity < 0.4 and model.tier == ModelTier.FREE:
            cost_score *= 1.3  # Boost free models for simple tasks

        # Historical performance boost
        historical_boost = self._get_historical_performance_boost(model.name, task_type)

        # Weighted composite score
        composite_score = (
            quality_score * preferences.get("quality", 0.3)
            + speed_score * preferences.get("speed", 0.3)
            + cost_score * preferences.get("cost", 0.4)
            + task_score * 0.2
            + historical_boost * 0.1
# BRACKET_SURGEON: disabled
#         )

        return min(1.0, composite_score)  # Cap at 1.0

    def _get_historical_performance_boost(self, model_name: str, task_type: TaskType) -> float:
        """Get performance boost based on historical success"""
        cache_key = f"{model_name}_{task_type.value}"

        if cache_key in self.performance_cache:
            perf = self.performance_cache[cache_key]
            # Boost based on success rate and usage count
            return min(0.2, perf.get("success_rate", 0.5) * 0.2)

        return 0.0  # No historical data

    def _create_routing_result(
        self, model_name: str, messages: List[Dict], task_type: TaskType, reason: str
# BRACKET_SURGEON: disabled
#     ) -> Dict:
        """Create the final routing result with optimized parameters"""
        model = self.models.get(model_name)
        if not model:
            model_name = self.config.get("model_routing", {}).get("fallback_model", "route - llm")
            model = self.models.get(model_name)

        # Get optimized parameters from the optimizer
        preferences = {"cost": 0.4, "speed": 0.3, "quality": 0.3}
        if model and model.tier == ModelTier.FREE:
            preferences["cost"] = 0.1  # Don't worry about cost for free models
            preferences["quality"] = 0.5

        optimized_params = self.optimizer.optimize_request(messages, preferences)

        # Override with selected model
        optimized_params["model"] = model_name

        # Add routing metadata
        result = {
            "routing_decision": {
                "selected_model": model_name,
                "task_type": task_type.value,
                "reason": reason,
                "model_tier": model.tier.value if model else "unknown",
                "estimated_cost": model.cost_per_token if model else 0,
                "timestamp": datetime.now().isoformat(),
# BRACKET_SURGEON: disabled
#             },
            "optimized_params": optimized_params,
            "model_info": {
                "name": model_name,
                "tier": model.tier.value if model else "unknown",
                "cost_per_token": model.cost_per_token if model else 0,
                "max_tokens": model.max_tokens if model else 4096,
                "strengths": [s.value for s in model.strengths] if model else [],
# BRACKET_SURGEON: disabled
#             },
# BRACKET_SURGEON: disabled
#         }

        return result

    def _record_routing_decision(self, decision: Dict):
        """Record routing decision for learning and analytics"""
        self.routing_history.append(decision)

        # Keep only recent history (last 1000 decisions)
        if len(self.routing_history) > 1000:
            self.routing_history = self.routing_history[-1000:]

    def record_request_outcome(
        self,
        model_name: str,
        task_type: TaskType,
        success: bool,
        response_time: float,
        quality_rating: float = None,
# BRACKET_SURGEON: disabled
#     ):
        """Record the outcome of a routed request for learning"""
        cache_key = f"{model_name}_{task_type.value}"

        if cache_key not in self.performance_cache:
            self.performance_cache[cache_key] = {
                "success_count": 0,
                "total_count": 0,
                "avg_response_time": 0,
                "avg_quality": 0,
# BRACKET_SURGEON: disabled
#             }

        perf = self.performance_cache[cache_key]
        perf["total_count"] += 1

        if success:
            perf["success_count"] += 1

        # Update averages (exponential moving average)
        alpha = 0.1
        perf["avg_response_time"] = (1 - alpha) * perf["avg_response_time"] + alpha * response_time

        if quality_rating is not None:
            perf["avg_quality"] = (1 - alpha) * perf["avg_quality"] + alpha * quality_rating

        perf["success_rate"] = perf["success_count"] / perf["total_count"]

        self.logger.info(
            f"ðŸ“Š Updated performance for {model_name} on {task_type.value}: "
            f"success_rate={perf['success_rate']:.2f}, avg_time={perf['avg_response_time']:.2f}s"
# BRACKET_SURGEON: disabled
#         )

    def get_routing_analytics(self) -> Dict:
        """Get comprehensive routing analytics"""
        if not self.routing_history:
            return {"message": "No routing history available"}

        # Analyze routing patterns
        total_decisions = len(self.routing_history)

        # Model usage distribution
        model_usage = {}
        task_distribution = {}

        for decision in self.routing_history:
            model = decision["selected_model"]
            task = decision["task_type"]

            model_usage[model] = model_usage.get(model, 0) + 1
            task_distribution[task] = task_distribution.get(task, 0) + 1

        # Performance metrics
        avg_routing_time = sum(d["routing_time"] for d in self.routing_history) / total_decisions
        avg_complexity = sum(d["complexity"] for d in self.routing_history) / total_decisions

        return {
            "total_routing_decisions": total_decisions,
            "avg_routing_time": avg_routing_time,
            "avg_task_complexity": avg_complexity,
            "model_usage_distribution": model_usage,
            "task_type_distribution": task_distribution,
            "performance_cache": dict(self.performance_cache),
            "available_models": list(self.models.keys()),
            "model_tiers": {name: model.tier.value for name, model in self.models.items()},
# BRACKET_SURGEON: disabled
#         }

    def suggest_model_for_task(
        self, task_description: str, budget_constraint: float = None
# BRACKET_SURGEON: disabled
#     ) -> Dict:
        """Suggest the best model for a specific task description"""
        # Create a mock message to classify the task
        messages = [{"role": "user", "content": task_description}]

        constraints = {}
        if budget_constraint:
            constraints["max_cost_per_token"] = budget_constraint

        # Route the request
        result = self.route_request(messages, constraints=constraints)

        return {
            "recommended_model": result["routing_decision"]["selected_model"],
            "task_type": result["routing_decision"]["task_type"],
            "reason": result["routing_decision"]["reason"],
            "estimated_cost_per_token": result["model_info"]["cost_per_token"],
            "model_tier": result["model_info"]["tier"],
            "model_strengths": result["model_info"]["strengths"],
# BRACKET_SURGEON: disabled
#         }


# Usage example and testing
if __name__ == "__main__":
    # Initialize router
    router = ModelRouter()

    print("ðŸŽ¯ Model Router initialized")
    print(f"ðŸ“Š Available models: {list(router.models.keys())}")

    # Test routing for different tasks
    test_cases = [
        "Write a Python function to calculate fibonacci numbers",
        "What is the meaning of life?",
        "Solve this equation: 2x + 5 = 15",
        "Write a creative story about a robot",
        "Summarize the key points of machine learning",
        "Translate 'Hello world' to French",
# BRACKET_SURGEON: disabled
#     ]

    print("\\nðŸ§ª Testing routing decisions:")
    for i, task in enumerate(test_cases, 1):
        messages = [{"role": "user", "content": task}]
        result = router.route_request(messages)

        print(f"\\n{i}. Task: {task[:50]}...")
        print(f"   Selected: {result['routing_decision']['selected_model']}")
        print(f"   Task Type: {result['routing_decision']['task_type']}")
        print(f"   Tier: {result['model_info']['tier']}")
        print(f"   Cost/token: ${result['model_info']['cost_per_token']:.4f}")

        # Simulate request outcome
        router.record_request_outcome(
            result["routing_decision"]["selected_model"],
            TaskType(result["routing_decision"]["task_type"]),
            success=True,
            response_time=2.0,
            quality_rating=0.85,
# BRACKET_SURGEON: disabled
#         )

    # Show analytics
    analytics = router.get_routing_analytics()
    print("\\nðŸ“ˆ Routing Analytics:")
    print(f"   Total decisions: {analytics['total_routing_decisions']}")
    print(f"   Avg routing time: {analytics['avg_routing_time']:.3f}s")
    print(f"   Model usage: {analytics['model_usage_distribution']}")
    print(f"   Task distribution: {analytics['task_type_distribution']}")

    # Test model suggestion
    suggestion = router.suggest_model_for_task(
        "Help me debug this complex algorithm", budget_constraint=0.005
# BRACKET_SURGEON: disabled
#     )
    print(f"\\nðŸ’¡ Model suggestion: {suggestion}")