# Prometheus AlertManager Configuration
# Comprehensive alerting system for real-time performance monitoring and scaling events

global:
  # SMTP configuration for email alerts
  smtp_smarthost: '${SMTP_HOST}:${SMTP_PORT}'
  smtp_from: '${ALERT_EMAIL_FROM}'
  smtp_auth_username: '${SMTP_USERNAME}'
  smtp_auth_password: '${SMTP_PASSWORD}'
  smtp_require_tls: true
  
  # Slack configuration
  slack_api_url: '${SLACK_WEBHOOK_URL}'
  
  # Default notification settings
  resolve_timeout: 5m

# Notification templates
templates:
  - '/etc/alertmanager/templates/*.tmpl'

# Routing configuration
route:
  # Default receiver for all alerts
  receiver: 'default-notifications'
  
  # Group alerts by service and alertname
  group_by: ['service', 'alertname', 'severity']
  
  # Wait time before sending initial notification
  group_wait: 30s
  
  # Wait time before sending additional notifications for the same group
  group_interval: 5m
  
  # Wait time before sending repeat notifications
  repeat_interval: 12h
  
  # Routing rules for different alert types
  routes:
    # Critical alerts - immediate notification
    - match:
        severity: critical
      receiver: 'critical-alerts'
      group_wait: 10s
      group_interval: 2m
      repeat_interval: 1h
      continue: true
    
    # High severity alerts
    - match:
        severity: high
      receiver: 'high-priority-alerts'
      group_wait: 30s
      group_interval: 5m
      repeat_interval: 4h
      continue: true
    
    # Scaling events
    - match:
        category: scaling
      receiver: 'scaling-notifications'
      group_wait: 1m
      group_interval: 10m
      repeat_interval: 24h
    
    # Performance alerts
    - match:
        category: performance
      receiver: 'performance-alerts'
      group_wait: 2m
      group_interval: 15m
      repeat_interval: 6h
    
    # Infrastructure alerts
    - match:
        category: infrastructure
      receiver: 'infrastructure-alerts'
      group_wait: 1m
      group_interval: 10m
      repeat_interval: 8h
    
    # Security alerts
    - match:
        category: security
      receiver: 'security-alerts'
      group_wait: 5s
      group_interval: 1m
      repeat_interval: 30m

# Inhibition rules to prevent alert spam
inhibit_rules:
  # Inhibit warning alerts when critical alerts are firing
  - source_match:
      severity: 'critical'
    target_match:
      severity: 'warning'
    equal: ['service', 'instance']
  
  # Inhibit scaling alerts when service is down
  - source_match:
      alertname: 'ServiceDown'
    target_match:
      category: 'scaling'
    equal: ['service']
  
  # Inhibit performance alerts during scaling events
  - source_match:
      category: 'scaling'
    target_match:
      category: 'performance'
    equal: ['service']

# Notification receivers
receivers:
  # Default notifications (Slack only)
  - name: 'default-notifications'
    slack_configs:
      - api_url: '${SLACK_WEBHOOK_URL}'
        channel: '#alerts'
        title: 'TRAE AI Alert'
        text: |
          {{ range .Alerts }}
          *Alert:* {{ .Annotations.summary }}
          *Description:* {{ .Annotations.description }}
          *Service:* {{ .Labels.service }}
          *Severity:* {{ .Labels.severity }}
          *Status:* {{ .Status }}
          {{ end }}
        color: '{{ if eq .Status "firing" }}danger{{ else }}good{{ end }}'
        send_resolved: true
  
  # Critical alerts (Slack + Email + PagerDuty)
  - name: 'critical-alerts'
    slack_configs:
      - api_url: '${SLACK_WEBHOOK_URL}'
        channel: '#critical-alerts'
        title: 'üö® CRITICAL ALERT - TRAE AI'
        text: |
          {{ range .Alerts }}
          *CRITICAL ALERT*
          *Service:* {{ .Labels.service }}
          *Alert:* {{ .Annotations.summary }}
          *Description:* {{ .Annotations.description }}
          *Runbook:* {{ .Annotations.runbook_url }}
          *Time:* {{ .StartsAt.Format "2006-01-02 15:04:05" }}
          {{ end }}
        color: 'danger'
        send_resolved: true
    
    email_configs:
      - to: '${CRITICAL_ALERT_EMAIL}'
        subject: 'üö® CRITICAL: {{ .GroupLabels.service }} - {{ .GroupLabels.alertname }}'
        body: |
          Critical alert detected in TRAE AI system:
          
          {{ range .Alerts }}
          Service: {{ .Labels.service }}
          Alert: {{ .Annotations.summary }}
          Description: {{ .Annotations.description }}
          Severity: {{ .Labels.severity }}
          Started: {{ .StartsAt.Format "2006-01-02 15:04:05" }}
          
          Runbook: {{ .Annotations.runbook_url }}
          Dashboard: {{ .Annotations.dashboard_url }}
          {{ end }}
        headers:
          Priority: 'high'
    
    pagerduty_configs:
      - routing_key: '${PAGERDUTY_INTEGRATION_KEY}'
        description: '{{ .GroupLabels.service }}: {{ .GroupLabels.alertname }}'
        severity: 'critical'
        details:
          service: '{{ .GroupLabels.service }}'
          alert: '{{ .GroupLabels.alertname }}'
          description: '{{ range .Alerts }}{{ .Annotations.description }}{{ end }}'
  
  # High priority alerts (Slack + Email)
  - name: 'high-priority-alerts'
    slack_configs:
      - api_url: '${SLACK_WEBHOOK_URL}'
        channel: '#high-priority-alerts'
        title: '‚ö†Ô∏è High Priority Alert - TRAE AI'
        text: |
          {{ range .Alerts }}
          *HIGH PRIORITY ALERT*
          *Service:* {{ .Labels.service }}
          *Alert:* {{ .Annotations.summary }}
          *Description:* {{ .Annotations.description }}
          *Time:* {{ .StartsAt.Format "2006-01-02 15:04:05" }}
          {{ end }}
        color: 'warning'
        send_resolved: true
    
    email_configs:
      - to: '${HIGH_PRIORITY_EMAIL}'
        subject: '‚ö†Ô∏è HIGH: {{ .GroupLabels.service }} - {{ .GroupLabels.alertname }}'
        body: |
          High priority alert in TRAE AI system:
          
          {{ range .Alerts }}
          Service: {{ .Labels.service }}
          Alert: {{ .Annotations.summary }}
          Description: {{ .Annotations.description }}
          Started: {{ .StartsAt.Format "2006-01-02 15:04:05" }}
          {{ end }}
  
  # Scaling notifications (Slack only)
  - name: 'scaling-notifications'
    slack_configs:
      - api_url: '${SLACK_WEBHOOK_URL}'
        channel: '#scaling-events'
        title: 'üìà Scaling Event - TRAE AI'
        text: |
          {{ range .Alerts }}
          *Scaling Event*
          *Service:* {{ .Labels.service }}
          *Event:* {{ .Annotations.summary }}
          *Details:* {{ .Annotations.description }}
          *Current Replicas:* {{ .Labels.current_replicas }}
          *Target Replicas:* {{ .Labels.target_replicas }}
          *Reason:* {{ .Labels.scaling_reason }}
          {{ end }}
        color: '{{ if eq .Labels.scaling_direction "up" }}good{{ else }}warning{{ end }}'
        send_resolved: false
  
  # Performance alerts (Slack)
  - name: 'performance-alerts'
    slack_configs:
      - api_url: '${SLACK_WEBHOOK_URL}'
        channel: '#performance-alerts'
        title: '‚ö° Performance Alert - TRAE AI'
        text: |
          {{ range .Alerts }}
          *Performance Issue Detected*
          *Service:* {{ .Labels.service }}
          *Metric:* {{ .Labels.metric_name }}
          *Current Value:* {{ .Labels.metric_value }}
          *Threshold:* {{ .Labels.threshold }}
          *Description:* {{ .Annotations.description }}
          {{ end }}
        color: 'warning'
        send_resolved: true
  
  # Infrastructure alerts (Slack + Email)
  - name: 'infrastructure-alerts'
    slack_configs:
      - api_url: '${SLACK_WEBHOOK_URL}'
        channel: '#infrastructure-alerts'
        title: 'üèóÔ∏è Infrastructure Alert - TRAE AI'
        text: |
          {{ range .Alerts }}
          *Infrastructure Issue*
          *Component:* {{ .Labels.component }}
          *Alert:* {{ .Annotations.summary }}
          *Description:* {{ .Annotations.description }}
          *Instance:* {{ .Labels.instance }}
          {{ end }}
        color: 'warning'
        send_resolved: true
    
    email_configs:
      - to: '${INFRASTRUCTURE_EMAIL}'
        subject: 'üèóÔ∏è Infrastructure: {{ .GroupLabels.component }} - {{ .GroupLabels.alertname }}'
        body: |
          Infrastructure alert in TRAE AI system:
          
          {{ range .Alerts }}
          Component: {{ .Labels.component }}
          Alert: {{ .Annotations.summary }}
          Description: {{ .Annotations.description }}
          Instance: {{ .Labels.instance }}
          Started: {{ .StartsAt.Format "2006-01-02 15:04:05" }}
          {{ end }}
  
  # Security alerts (Slack + Email + PagerDuty)
  - name: 'security-alerts'
    slack_configs:
      - api_url: '${SLACK_WEBHOOK_URL}'
        channel: '#security-alerts'
        title: 'üîí SECURITY ALERT - TRAE AI'
        text: |
          {{ range .Alerts }}
          *SECURITY ALERT*
          *Type:* {{ .Labels.security_type }}
          *Alert:* {{ .Annotations.summary }}
          *Description:* {{ .Annotations.description }}
          *Source:* {{ .Labels.source_ip }}
          *Time:* {{ .StartsAt.Format "2006-01-02 15:04:05" }}
          {{ end }}
        color: 'danger'
        send_resolved: true
    
    email_configs:
      - to: '${SECURITY_EMAIL}'
        subject: 'üîí SECURITY: {{ .GroupLabels.security_type }} - {{ .GroupLabels.alertname }}'
        body: |
          SECURITY ALERT in TRAE AI system:
          
          {{ range .Alerts }}
          Type: {{ .Labels.security_type }}
          Alert: {{ .Annotations.summary }}
          Description: {{ .Annotations.description }}
          Source IP: {{ .Labels.source_ip }}
          Started: {{ .StartsAt.Format "2006-01-02 15:04:05" }}
          
          Immediate action may be required.
          {{ end }}
        headers:
          Priority: 'urgent'
    
    pagerduty_configs:
      - routing_key: '${PAGERDUTY_SECURITY_KEY}'
        description: 'Security Alert: {{ .GroupLabels.security_type }}'
        severity: 'critical'
        details:
          type: '{{ .GroupLabels.security_type }}'
          source: '{{ .GroupLabels.source_ip }}'
          description: '{{ range .Alerts }}{{ .Annotations.description }}{{ end }}'

# Silence configuration
silences:
  # Example: Silence maintenance window alerts
  # This would typically be managed via API or UI
  comment: 'Configured via alertmanager.yml'

# Time intervals for different notification schedules
time_intervals:
  - name: 'business-hours'
    time_intervals:
      - times:
          - start_time: '09:00'
            end_time: '17:00'
        weekdays: ['monday:friday']
        location: 'UTC'
  
  - name: 'weekends'
    time_intervals:
      - times:
          - start_time: '00:00'
            end_time: '23:59'
        weekdays: ['saturday', 'sunday']
        location: 'UTC'
  
  - name: 'off-hours'
    time_intervals:
      - times:
          - start_time: '17:01'
            end_time: '08:59'
        weekdays: ['monday:friday']
        location: 'UTC'
      - times:
          - start_time: '00:00'
            end_time: '23:59'
        weekdays: ['saturday', 'sunday']
        location: 'UTC'

# Mute time intervals (when not to send alerts)
mute_time_intervals:
  - name: 'maintenance-window'
    time_intervals:
      - times:
          - start_time: '02:00'
            end_time: '04:00'
        weekdays: ['sunday']
        location: 'UTC'

# Active time intervals (when to send alerts)
active_time_intervals:
  - name: 'always'
    time_intervals:
      - times:
          - start_time: '00:00'
            end_time: '23:59'
        weekdays: ['monday:sunday']
        location: 'UTC'