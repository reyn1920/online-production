#!/usr/bin/env python3
""""""
TRAE.AI MacBook Air M1 Performance Optimizer

Intelligent task serialization and dynamic resource throttling specifically
optimized for Apple Silicon M1 architecture. Provides adaptive performance
management with thermal awareness and energy efficiency optimization.

Features:
- M1 - specific CPU core management (Performance + Efficiency cores)
- Thermal throttling prevention
- Intelligent task serialization
- Dynamic resource allocation
- Memory pressure monitoring
- Energy efficiency optimization
- Background task prioritization

Author: TRAE.AI System
Version: 1.0.0
""""""

import json
import logging
import os
import platform
import subprocess
import threading
import time
from collections import deque
from concurrent.futures import ThreadPoolExecutor
from dataclasses import dataclass, field
from datetime import datetime
from enum import Enum
from typing import Any, Callable, Dict, Optional

import psutil

# Import our logger
try:

    from utils.logger import get_logger

except ImportError:

    import logging

    logging.basicConfig(level = logging.INFO)
    def get_logger(name):
        return logging.getLogger(name)

logger = get_logger(__name__)


class TaskPriority(Enum):
    """Task priority levels for M1 optimization."""

    CRITICAL = 1
    HIGH = 2
    MEDIUM = 3
    LOW = 4
    BACKGROUND = 5


class CoreType(Enum):
    """M1 CPU core types."""

    PERFORMANCE = "performance"
    EFFICIENCY = "efficiency"
    AUTO = "auto"

@dataclass


class M1SystemMetrics:
    """M1 - specific system metrics."""

    timestamp: datetime
    cpu_percent: float
    performance_cores_usage: float
    efficiency_cores_usage: float
    memory_percent: float
    memory_pressure: str  # "normal", "warn", "critical"
    thermal_state: str  # "nominal", "fair", "serious", "critical"
    energy_impact: float  # Energy impact score
    swap_usage_mb: float
    gpu_usage: float
    neural_engine_usage: float
    active_tasks: int
    background_tasks: int

@dataclass


class OptimizationTask:
    """Task for M1 optimization queue."""

    id: str
    name: str
    priority: TaskPriority
    core_preference: CoreType
    estimated_duration: float
    memory_requirement: int  # MB
    cpu_intensive: bool
    gpu_required: bool
    neural_engine_required: bool
    callback: Callable
    args: tuple = field(default_factory = tuple)
    kwargs: dict = field(default_factory = dict)
    created_at: datetime = field(default_factory = datetime.now)
    started_at: Optional[datetime] = None
    completed_at: Optional[datetime] = None
    result: Any = None
    error: Optional[str] = None


class M1PerformanceOptimizer:
    """MacBook Air M1 Performance Optimizer with intelligent task management."""


    def __init__(self, config: Optional[Dict[str, Any]] = None):
        self.config = config or self._get_default_config()
        self.is_m1 = self._detect_m1_system()

        # Task management
        self.task_queue = deque()
        self.active_tasks = {}
        self.completed_tasks = deque(maxlen = 1000)
        self.task_lock = threading.Lock()

        # Performance monitoring
        self.metrics_history = deque(maxlen = 300)  # 5 minutes at 1 - second intervals
        self.performance_baseline = None
        self.thermal_history = deque(maxlen = 60)  # 1 minute thermal history

        # Resource management
        self.max_concurrent_tasks = self._calculate_optimal_concurrency()
        self.current_throttle_level = 0  # 0 - 5 throttle levels
        self.energy_mode = "balanced"  # "performance", "balanced", "efficiency"

        # Threading
        self.monitor_thread = None
        self.scheduler_thread = None
        self.running = False

        # Executors for different core types
        self.performance_executor = ThreadPoolExecutor(
            max_workers=self._get_performance_core_count(),
            thread_name_prefix="M1-Perf"
# BRACKET_SURGEON: disabled
#         )
        self.efficiency_executor = ThreadPoolExecutor(
            max_workers=self._get_efficiency_core_count(),
            thread_name_prefix="M1-Eff"
# BRACKET_SURGEON: disabled
#         )

        logger.info(f"M1 Optimizer initialized - M1 System: {self.is_m1}")
        logger.info(f"Max concurrent tasks: {self.max_concurrent_tasks}")


    def _get_default_config(self) -> Dict[str, Any]:
        """Get default M1 optimization configuration."""
        return {
            "thermal_throttle_threshold": 80,  # Â°C
            "memory_pressure_threshold": 85,  # %
            "cpu_throttle_threshold": 90,  # %
            "energy_efficiency_mode": True,
            "background_task_limit": 3,
            "performance_monitoring_interval": 1.0,  # seconds
            "task_timeout": 300,  # seconds
            "adaptive_throttling": True,
            "neural_engine_optimization": True,
            "gpu_task_scheduling": True,
# BRACKET_SURGEON: disabled
#         }


    def _detect_m1_system(self) -> bool:
        """Detect if running on M1 MacBook Air."""
        try:
            if platform.system() != "Darwin":
                return False

            # Check for Apple Silicon
            result = subprocess.run(
                ["sysctl", "-n", "machdep.cpu.brand_string"],
                capture_output=True,
                text=True,
# BRACKET_SURGEON: disabled
#             )

            cpu_brand = result.stdout.strip()
            is_apple_silicon = "Apple" in cpu_brand

            if is_apple_silicon:
                logger.info(f"Detected Apple Silicon: {cpu_brand}")

            return is_apple_silicon

        except Exception as e:
            logger.warning(f"Could not detect M1 system: {e}")
            return False


    def _get_performance_core_count(self) -> int:
        """Get number of performance cores (M1 has 4)."""
        if self.is_m1:
            return 4  # M1 has 4 performance cores
        return max(1, psutil.cpu_count()//2)


    def _get_efficiency_core_count(self) -> int:
        """Get number of efficiency cores (M1 has 4)."""
        if self.is_m1:
            return 4  # M1 has 4 efficiency cores
        return max(1, psutil.cpu_count()//2)


    def _calculate_optimal_concurrency(self) -> int:
        """Calculate optimal concurrent task limit for M1."""
        if self.is_m1:
            # M1-specific optimization
            memory_gb = psutil.virtual_memory().total/(1024**3)

            if memory_gb >= 16:
                return 8  # High-end M1
            elif memory_gb >= 8:
                return 6  # Standard M1
            else:
                return 4  # Base M1

        # Fallback for non-M1 systems
        return min(8, psutil.cpu_count())


    def get_system_metrics(self) -> M1SystemMetrics:
        """Collect comprehensive M1 system metrics."""
        try:
            # Basic CPU and memory
            cpu_percent = psutil.cpu_percent(interval = 0.1)
            memory = psutil.virtual_memory()
            swap = psutil.swap_memory()

            # M1-specific metrics
            performance_cores_usage = self._get_performance_cores_usage()
            efficiency_cores_usage = self._get_efficiency_cores_usage()
            thermal_state = self._get_thermal_state()
            memory_pressure = self._get_memory_pressure()
            energy_impact = self._calculate_energy_impact()

            # GPU and Neural Engine (M1-specific)
            gpu_usage = self._get_gpu_usage()
            neural_engine_usage = self._get_neural_engine_usage()

            return M1SystemMetrics(
                timestamp=datetime.now(),
                cpu_percent=cpu_percent,
                performance_cores_usage=performance_cores_usage,
                efficiency_cores_usage=efficiency_cores_usage,
                memory_percent=memory.percent,
                memory_pressure=memory_pressure,
                thermal_state=thermal_state,
                energy_impact=energy_impact,
                swap_usage_mb=swap.used/(1024**2),
                gpu_usage=gpu_usage,
                neural_engine_usage=neural_engine_usage,
                active_tasks=len(self.active_tasks),
                background_tasks=sum(
                    1
                    for t in self.active_tasks.values()
                    if t.priority == TaskPriority.LOW
# BRACKET_SURGEON: disabled
#                 ),
# BRACKET_SURGEON: disabled
#             )

        except Exception as e:
            logger.error(f"Failed to collect system metrics: {e}")
            # Return minimal metrics on error
            return M1SystemMetrics(
                timestamp=datetime.now(),
                cpu_percent=0.0,
                performance_cores_usage=0.0,
                efficiency_cores_usage=0.0,
                memory_percent=0.0,
                memory_pressure="unknown",
                thermal_state="unknown",
                energy_impact=0.0,
                swap_usage_mb=0.0,
                gpu_usage=0.0,
                neural_engine_usage=0.0,
                active_tasks=0,
                background_tasks=0,
# BRACKET_SURGEON: disabled
#             )


    def _get_performance_cores_usage(self) -> float:
        """Get performance cores usage (M1-specific)."""
        if not self.is_m1:
            return psutil.cpu_percent()/2  # Estimate

        try:
            # Use powermetrics for M1-specific core usage
            subprocess.run(
                [
                    "sudo",
                    "powermetrics",
                    "-n",
                    "1",
                    "-i",
                    "100",
                    "--samplers",
                    "cpu_power",
# BRACKET_SURGEON: disabled
#                 ],
                capture_output=True,
                text=True,
                timeout=2,
# BRACKET_SURGEON: disabled
#             )

            # Parse performance core usage from output
            # This is a simplified implementation
            return psutil.cpu_percent() * 0.6  # Estimate 60% on performance cores

        except Exception:
            return psutil.cpu_percent() * 0.6  # Fallback estimate


    def _get_efficiency_cores_usage(self) -> float:
        """Get efficiency cores usage (M1-specific)."""
        if not self.is_m1:
            return psutil.cpu_percent()/2  # Estimate

        try:
            # Estimate efficiency core usage
            total_cpu = psutil.cpu_percent()
            perf_usage = self._get_performance_cores_usage()
            return max(0, total_cpu - perf_usage)

        except Exception:
            return psutil.cpu_percent() * 0.4  # Fallback estimate


    def _get_thermal_state(self) -> str:
        """Get thermal state of the system."""
        if not self.is_m1:
            return "unknown"

        try:
            # Use powermetrics to get thermal state
            result = subprocess.run(
                ["pmset", "-g", "therm"], capture_output=True, text=True, timeout=2
# BRACKET_SURGEON: disabled
#             )

            output = result.stdout.lower()
            if "critical" in output:
                return "critical"
            elif "serious" in output:
                return "serious"
            elif "fair" in output:
                return "fair"
            else:
                return "nominal"

        except Exception:
            # Fallback: estimate based on CPU usage
            cpu_usage = psutil.cpu_percent()
            if cpu_usage > 90:
                return "serious"
            elif cpu_usage > 70:
                return "fair"
            else:
                return "nominal"


    def _get_memory_pressure(self) -> str:
        """Get memory pressure state."""
        try:
            memory = psutil.virtual_memory()
            swap = psutil.swap_memory()

            # Calculate memory pressure
            if memory.percent > 95 or swap.percent > 50:
                return "critical"
            elif memory.percent > 85 or swap.percent > 25:
                return "warn"
            else:
                return "normal"

        except Exception:
            return "unknown"


    def _calculate_energy_impact(self) -> float:
        """Calculate energy impact score (0 - 100)."""
        try:
            cpu_usage = psutil.cpu_percent()
            memory_usage = psutil.virtual_memory().percent

            # Simple energy impact calculation
            # Higher CPU and memory usage = higher energy impact
            energy_impact = (cpu_usage * 0.7) + (memory_usage * 0.3)
            return min(100.0, energy_impact)

        except Exception:
            return 0.0


    def _get_gpu_usage(self) -> float:
        """Get GPU usage (M1-specific)."""
        if not self.is_m1:
            return 0.0

        try:
            # Use ioreg to get GPU usage on M1
            subprocess.run(
                ["ioreg", "-r", "-d", "1", "-w", "0", "-c", "IOAccelerator"],
                capture_output=True,
                text=True,
                timeout=2,
# BRACKET_SURGEON: disabled
#             )

            # This is a simplified implementation
            # Real implementation would parse ioreg output
            return 0.0  # Placeholder

        except Exception:
            return 0.0


    def _get_neural_engine_usage(self) -> float:
        """Get Neural Engine usage (M1-specific)."""
        if not self.is_m1:
            return 0.0

        # Neural Engine usage is not easily accessible
        # This would require more advanced system monitoring
        return 0.0  # Placeholder


    def add_task(
        self,
        name: str,
        callback: Callable,
        priority: TaskPriority = TaskPriority.MEDIUM,
        core_preference: CoreType = CoreType.AUTO,
        estimated_duration: float = 60.0,
        memory_requirement: int = 100,
        cpu_intensive: bool = False,
        gpu_required: bool = False,
        neural_engine_required: bool = False,
        *args,
        **kwargs,
# BRACKET_SURGEON: disabled
#     ) -> str:
        """Add a task to the optimization queue."""

        task_id = f"task_{int(time.time() * 1000)}_{len(self.task_queue)}"

        task = OptimizationTask(
            id=task_id,
            name=name,
            priority=priority,
            core_preference=core_preference,
            estimated_duration=estimated_duration,
            memory_requirement=memory_requirement,
            cpu_intensive=cpu_intensive,
            gpu_required=gpu_required,
            neural_engine_required=neural_engine_required,
            callback=callback,
            args=args,
            kwargs=kwargs,
# BRACKET_SURGEON: disabled
#         )

        with self.task_lock:
            # Insert task based on priority
            inserted = False
            for i, existing_task in enumerate(self.task_queue):
                if task.priority.value < existing_task.priority.value:
                    self.task_queue.insert(i, task)
                    inserted = True
                    break

            if not inserted:
                self.task_queue.append(task)

        logger.info(f"Added task {task_id}: {name} (Priority: {priority.name})")
        return task_id


    def start_optimization(self):
        """Start the M1 optimization system."""
        if self.running:
            logger.warning("Optimization already running")
            return

        self.running = True

        # Start monitoring thread
        self.monitor_thread = threading.Thread(
            target=self._monitoring_loop, name="M1-Monitor", daemon=True
# BRACKET_SURGEON: disabled
#         )
        self.monitor_thread.start()

        # Start scheduler thread
        self.scheduler_thread = threading.Thread(
            target=self._scheduler_loop, name="M1-Scheduler", daemon=True
# BRACKET_SURGEON: disabled
#         )
        self.scheduler_thread.start()

        logger.info("M1 Optimization system started")


    def stop_optimization(self):
        """Stop the M1 optimization system."""
        self.running = False

        if self.monitor_thread:
            self.monitor_thread.join(timeout=5)

        if self.scheduler_thread:
            self.scheduler_thread.join(timeout=5)

        # Shutdown executors
        self.performance_executor.shutdown(wait=True)
        self.efficiency_executor.shutdown(wait=True)

        logger.info("M1 Optimization system stopped")


    def _monitoring_loop(self):
        """Main monitoring loop for system metrics."""
        logger.info("Starting M1 monitoring loop")

        while self.running:
            try:
                # Collect metrics
                metrics = self.get_system_metrics()
                self.metrics_history.append(metrics)

                # Update thermal history
                self.thermal_history.append(
                    {
                        "timestamp": metrics.timestamp,
                        "thermal_state": metrics.thermal_state,
                        "cpu_percent": metrics.cpu_percent,
# BRACKET_SURGEON: disabled
#                     }
# BRACKET_SURGEON: disabled
#                 )

                # Adaptive throttling based on conditions
                if self.config["adaptive_throttling"]:
                    self._update_throttle_level(metrics)

                # Log metrics periodically
                if len(self.metrics_history) % 30 == 0:  # Every 30 seconds
                    self._log_performance_summary(metrics)

                time.sleep(self.config["performance_monitoring_interval"])

            except Exception as e:
                logger.error(f"Monitoring loop error: {e}")
                time.sleep(5)


    def _scheduler_loop(self):
        """Main scheduler loop for task execution."""
        logger.info("Starting M1 task scheduler loop")

        while self.running:
            try:
                # Check if we can run more tasks
                if len(self.active_tasks) >= self.max_concurrent_tasks:
                    time.sleep(0.5)
                    continue

                # Get next task
                task = self._get_next_task()
                if not task:
                    time.sleep(1)
                    continue

                # Execute task
                self._execute_task(task)

            except Exception as e:
                logger.error(f"Scheduler loop error: {e}")
                time.sleep(5)


    def _get_next_task(self) -> Optional[OptimizationTask]:
        """Get the next task to execute based on current system state."""
        with self.task_lock:
            if not self.task_queue:
                return None

            # Get current metrics
            current_metrics = self.get_system_metrics()

            # Find suitable task based on system state
            for i, task in enumerate(self.task_queue):
                if self._can_execute_task(task, current_metrics):
                    return (
                        self.task_queue.popleft() if i == 0 else self.task_queue.pop(i)
# BRACKET_SURGEON: disabled
#                     )

            return None


    def _can_execute_task(
        self, task: OptimizationTask, metrics: M1SystemMetrics
# BRACKET_SURGEON: disabled
#     ) -> bool:
        """Check if a task can be executed given current system state."""
        # Check memory requirements
        available_memory = (
            (100 - metrics.memory_percent)
            * psutil.virtual_memory().total / (100 * 1024**2)
# BRACKET_SURGEON: disabled
#         )
        if task.memory_requirement > available_memory:
            return False

        # Check thermal state
        if metrics.thermal_state in ["serious", "critical"] and task.cpu_intensive:
            return False

        # Check memory pressure
        if metrics.memory_pressure == "critical" and task.memory_requirement > 500:
            return False

        # Check background task limits
        if (
            task.priority == TaskPriority.LOW
            and metrics.background_tasks >= self.config["background_task_limit"]
# BRACKET_SURGEON: disabled
#         ):
            return False

        return True


    def _execute_task(self, task: OptimizationTask):
        """Execute a task on the appropriate executor."""
        task.started_at = datetime.now()
        self.active_tasks[task.id] = task

        # Choose executor based on task characteristics and core preference
        executor = self._choose_executor(task)

        # Submit task
        future = executor.submit(self._run_task, task)
        future.add_done_callback(lambda f: self._task_completed(task.id, f))

        logger.info(
            f"Executing task {task.id}: {task.name} on {executor._thread_name_prefix} cores"
# BRACKET_SURGEON: disabled
#         )


    def _choose_executor(self, task: OptimizationTask) -> ThreadPoolExecutor:
        """Choose the appropriate executor for a task."""
        if task.core_preference == CoreType.PERFORMANCE:
            return self.performance_executor
        elif task.core_preference == CoreType.EFFICIENCY:
            return self.efficiency_executor
        else:
            # Auto-choose based on task characteristics
            if task.cpu_intensive or task.priority in [
                TaskPriority.CRITICAL,
                TaskPriority.HIGH,
# BRACKET_SURGEON: disabled
#             ]:
                return self.performance_executor
            else:
                return self.efficiency_executor


    def _run_task(self, task: OptimizationTask) -> Any:
        """Run a task with timeout and error handling."""
        try:
            # Set thread priority based on task priority
            self._set_thread_priority(task.priority)

            # Execute the task
            result = task.callback(*task.args, **task.kwargs)
            return result

        except Exception as e:
            logger.error(f"Task {task.id} failed: {e}")
            raise


    def _set_thread_priority(self, priority: TaskPriority):
        """Set thread priority based on task priority."""
        try:
            if priority == TaskPriority.CRITICAL:
                os.nice(-5)  # Higher priority
            elif priority == TaskPriority.LOW:
                os.nice(10)  # Lower priority
        except Exception:
            pass  # Ignore if we can't set priority


    def _task_completed(self, task_id: str, future):
        """Handle task completion."""
        task = self.active_tasks.pop(task_id, None)
        if not task:
            return

        task.completed_at = datetime.now()

        try:
            task.result = future.result()
            logger.info(f"Task {task_id} completed successfully")
        except Exception as e:
            task.error = str(e)
            logger.error(f"Task {task_id} failed: {e}")

        self.completed_tasks.append(task)


    def _update_throttle_level(self, metrics: M1SystemMetrics):
        """Update throttle level based on system metrics."""
        old_level = self.current_throttle_level

        # Increase throttle if system is under stress
        if (
            metrics.thermal_state == "critical"
            or metrics.memory_pressure == "critical"
            or metrics.cpu_percent > 95
# BRACKET_SURGEON: disabled
#         ):
            self.current_throttle_level = min(5, self.current_throttle_level + 1)

        elif (
            metrics.thermal_state == "serious"
            or metrics.memory_pressure == "warn"
            or metrics.cpu_percent > 85
# BRACKET_SURGEON: disabled
#         ):
            self.current_throttle_level = min(4, self.current_throttle_level + 1)

        # Decrease throttle if system is stable
        elif (
            metrics.thermal_state == "nominal"
            and metrics.memory_pressure == "normal"
            and metrics.cpu_percent < 60
# BRACKET_SURGEON: disabled
#         ):
            self.current_throttle_level = max(0, self.current_throttle_level - 1)

        # Apply throttle changes
        if self.current_throttle_level != old_level:
            self._apply_throttle_level()
            logger.info(
                f"Throttle level changed: {old_level} -> {self.current_throttle_level}"
# BRACKET_SURGEON: disabled
#             )


    def _apply_throttle_level(self):
        """Apply current throttle level to system."""
        # Adjust max concurrent tasks based on throttle level
        base_concurrency = self._calculate_optimal_concurrency()
        throttle_factor = max(0.2, 1.0 - (self.current_throttle_level * 0.15))
        self.max_concurrent_tasks = max(1, int(base_concurrency * throttle_factor))

        logger.debug(
            f"Applied throttle level {self.current_throttle_level}: "
            f"max_concurrent_tasks = {self.max_concurrent_tasks}"
# BRACKET_SURGEON: disabled
#         )


    def _log_performance_summary(self, metrics: M1SystemMetrics):
        """Log performance summary."""
        logger.info(
            f"M1 Performance - CPU: {metrics.cpu_percent:.1f}% "
            f"(P-cores: {metrics.performance_cores_usage:.1f}%, "
            f"E-cores: {metrics.efficiency_cores_usage:.1f}%), "
            f"Memory: {metrics.memory_percent:.1f}% ({metrics.memory_pressure}), "
            f"Thermal: {metrics.thermal_state}, "
            f"Tasks: {metrics.active_tasks}/{self.max_concurrent_tasks}, "
            f"Throttle: {self.current_throttle_level}/5"
# BRACKET_SURGEON: disabled
#         )


    def get_performance_report(self) -> Dict[str, Any]:
        """Get comprehensive performance report."""
        if not self.metrics_history:
            return {"error": "No metrics available"}

        latest_metrics = self.metrics_history[-1]

        # Calculate averages over last 5 minutes
        recent_metrics = list(self.metrics_history)[-60:]  # Last 60 seconds

        avg_cpu = sum(m.cpu_percent for m in recent_metrics) / len(recent_metrics)
        avg_memory = sum(m.memory_percent for m in recent_metrics) / len(recent_metrics)
        avg_energy = sum(m.energy_impact for m in recent_metrics) / len(recent_metrics)

        return {
            "timestamp": latest_metrics.timestamp.isoformat(),
            "system_info": {
                "is_m1": self.is_m1,
                "performance_cores": self._get_performance_core_count(),
                "efficiency_cores": self._get_efficiency_core_count(),
# BRACKET_SURGEON: disabled
#             },
            "current_metrics": {
                "cpu_percent": latest_metrics.cpu_percent,
                "performance_cores_usage": latest_metrics.performance_cores_usage,
                "efficiency_cores_usage": latest_metrics.efficiency_cores_usage,
                "memory_percent": latest_metrics.memory_percent,
                "memory_pressure": latest_metrics.memory_pressure,
                "thermal_state": latest_metrics.thermal_state,
                "energy_impact": latest_metrics.energy_impact,
# BRACKET_SURGEON: disabled
#             },
            "averages_5min": {
                "cpu_percent": avg_cpu,
                "memory_percent": avg_memory,
                "energy_impact": avg_energy,
# BRACKET_SURGEON: disabled
#             },
            "task_management": {
                "active_tasks": len(self.active_tasks),
                "queued_tasks": len(self.task_queue),
                "completed_tasks": len(self.completed_tasks),
                "max_concurrent_tasks": self.max_concurrent_tasks,
                "current_throttle_level": self.current_throttle_level,
# BRACKET_SURGEON: disabled
#             },
            "optimization_status": {
                "running": self.running,
                "energy_mode": self.energy_mode,
                "adaptive_throttling": self.config["adaptive_throttling"],
# BRACKET_SURGEON: disabled
#             },
# BRACKET_SURGEON: disabled
#         }

# Global optimizer instance
_optimizer_instance = None


def get_m1_optimizer(config: Optional[Dict[str, Any]] = None) -> M1PerformanceOptimizer:
    """Get global M1 optimizer instance."""
    global _optimizer_instance

    if _optimizer_instance is None:
        _optimizer_instance = M1PerformanceOptimizer(config)

    return _optimizer_instance


def optimize_for_m1(func: Callable) -> Callable:
    """Decorator to optimize function execution for M1."""


    def wrapper(*args, **kwargs):
        optimizer = get_m1_optimizer()

        # Add function as a task
        task_id = optimizer.add_task(
            name=func.__name__,
            callback=func,
            priority=TaskPriority.MEDIUM,
            core_preference=CoreType.AUTO,
            *args,
            **kwargs,
# BRACKET_SURGEON: disabled
#         )

        # Wait for completion (simplified - in real use, this would be async)
        while task_id in optimizer.active_tasks:
            time.sleep(0.1)

        # Find completed task
        for task in optimizer.completed_tasks:
            if task.id == task_id:
                if task.error:
                    raise Exception(task.error)
                return task.result

        raise Exception("Task not found in completed tasks")

    return wrapper

# Example usage and testing
if __name__ == "__main__":
    # Initialize optimizer
    optimizer = M1PerformanceOptimizer()

    # Start optimization
    optimizer.start_optimization()

    # Add some test tasks


    def cpu_intensive_task(duration=5):
        """Simulate CPU-intensive task."""
        start_time = time.time()
        while time.time() - start_time < duration:
            # Simulate work
            sum(i**2 for i in range(1000))
        return f"CPU task completed in {duration}s"


    def memory_intensive_task(size_mb=100):
        """Simulate memory-intensive task."""
        data = [0] * (size_mb * 1024 * 256)  # Allocate memory
        time.sleep(2)
        del data
        return f"Memory task completed with {size_mb}MB"

    # Add tasks
    optimizer.add_task(
        "CPU Task 1",
        cpu_intensive_task,
        priority=TaskPriority.HIGH,
        core_preference=CoreType.PERFORMANCE,
        cpu_intensive=True,
        args=(3,),
# BRACKET_SURGEON: disabled
#     )

    optimizer.add_task(
        "Memory Task 1",
        memory_intensive_task,
        priority=TaskPriority.MEDIUM,
        core_preference=CoreType.EFFICIENCY,
        memory_requirement=200,
        args=(150,),
# BRACKET_SURGEON: disabled
#     )

    # Run for a while
    try:
        time.sleep(30)

        # Get performance report
        report = optimizer.get_performance_report()
        print(json.dumps(report, indent=2, default=str))

    finally:
        optimizer.stop_optimization()